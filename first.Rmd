---
title: "STAT253"
author: "Federico Chung, KY, Niketh Gamage, Josh Upadhyay"
date: "10/3/2019"
output: html_document
---
---
title: "Mini-Project 1"
author: "Federico Chung, KY, Niketh Gamage, Josh Upadhyay"
output: 
  html_document:
    toc: true
    toc_float: true
---




\
\



## Part 1: Ready the data


\
\
\
\
\
\

```{r setup, include=FALSE}
library(ggplot2)  # for plots
library(dplyr)    # for wrangling
library(caret)    # for machine learning algorithms
library(tidyr)    # because
library(naniar)   # we need it 
library(RANN)
```


```{r}
data<- read.csv("airbnb_joined.csv")

```

```{r}
data<-data%>%
  select(-c(X))
```



**Sampling the data**
```{r}
set.seed(256)

data<-data%>%
  sample_n(5000)

data

data<- data %>% filter(price<1000)

```

**Possible irrelevant variables**
```{r}
colnames(data)
```

Based on the names of the variables, we believe id, X, and host_response_rate2 are probably the least important variables. We also notice multiple variables with seemingly similar (correlated) information, like host response rate, bathrooms, and reviews, which all have a second variable. We'd probably remove the second variable as it likely doesn't add too much new information. 


```{r}
names(data)
na_strings <- c("NA", "N A", "N / A", "N/A", "N/ A", "Not Available", "NOt available")

data<-data%>%
  replace_with_na_all(condition = ~.x %in% na_strings)


data <- data %>%
  mutate(bathrooms2 = replace_na(bathrooms, mean(bathrooms, na.rm = TRUE)))%>%
  mutate(beds2 = replace_na(beds, mean(beds, na.rm = TRUE)))%>%
  mutate(reviews2 = replace_na(reviews_per_month, mean(reviews_per_month, na.rm = TRUE)))%>%
  mutate(host_response_rate2 = replace_na(host_response_rate, mean(host_response_rate, na.rm = TRUE)))
```


Using the colSums function, we determined which variables had many NAs (in which case we removed, like square_feet), or could be salvaged by computing their mean. We also removed a few additional variables we deemed unimportant (lat/longitude).

```{r}

data_small<- data%>% select(-c(id, is_location_exact, calendar_updated, latitude, longitude, square_feet))
sum(complete.cases(data_small))


colSums(is.na(data))

colSums(is.na(data_small))

```


We then ran knnImpute on the remaining variables. 
```{r}
data_small<- data_small%>%
  select(-c(price))

impute_info <- data_small %>%
  preProcess(method = "knnImpute")

data_complete <- predict(impute_info, newdata = data_small) 

```

```{r}
colSums(is.na(data_complete))
sum(complete.cases(data_complete))
```

Now we have 0 NAs!

#Narrowing Down the Predictors using LASSO: 
```{r}
    # Define a range of lambda tuning parameters to try
    lambda_grid <- 10^seq(-3, 0.5, length = 100)

    # Set the seed
    set.seed(253)

    # Perform LASSO
    lasso_model_data <- train(
      price~.,
      data = data_complete,
      method = "glmnet",
      trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
      tuneGrid = data.frame(alpha=1, lambda=lambda_grid),
      metric = "MAE",
      na.action = na.omit
    )
    
```
    
```{r}
    # Plot coefficients for each LASSO
plot(lasso_model_data$finalModel, xvar = "lambda", label = TRUE, col = rainbow(20))
    # Codebook for which variables the numbers correspond to

coef(lasso_model_data$finalModel, lasso_model_data$bestTune$lambda)

final_variables<- c("host_response_time","neighbourhood_cleansed","room_type","accommodates","bathrooms","bedrooms","amenities","minimum_nights","maximum_nights","availability_30","number_of_reviews","review_scores_rating","is_business_travel_ready","reviews_per_month","neighbourhood_group" )
``` 


**Based on the results of LASSO, we reduced our 'small' dataset even further into 20 final variables. We believe this dimensionality reduction was necessary to reduce computing power, and also remove unnecessary or correlated variables (notice how reviews has been removed, but reviews2 is still present). 


**added price back into the dataset, as it's what we're trying to predict!**
```{r}
data_new<-data_complete%>%
  mutate(price = data$price)

data_final <- data_new%>%
  select(c(final_variables, price))

```

    
```{r}
# Identify which tuning parameter (ie. lambda) is "best" 
lasso_model_data$bestTune
``` 

## Part 2: Analyze


\
\
\
\
\
\



# make some graphs relating price to different variables


```{r}
glimpse(data_final)

data_final %>%
  ggplot(aes(x = host_response_time, y=reviews_per_month)) + geom_point()
```
Interestingly, there's no relationship between host response rate and number of reviews per month. We're not suprised to see a spike in reviews on nonresponsive and highly responsive hosts, though, as people may leave many negative reviews on bad hosts (almost 0 response rate) or very positive responses for great hosts. 


```{r}
data_final%>%
  group_by(property_type) %>%
  summarize(mean_reviews = mean(reviews2))%>%
  ggplot(aes(property_type, mean_reviews)) + geom_bar(stat = 'identity') + 
  theme_minimal() + labs(title = "Are Some Property Types More Valued Than Others?",
                         x = "Property Type", y = "mean(Reviews)")
  
```

```{r}
data_final %>%
  group_by(property_type)%>%
  summarize(tot = n()) %>%
  arrange(desc(tot))%>%
  head(5)
```
Clearly there's also an imbalance in the number of listings per property type, but it's interesting that 1 is rated so low on average even though it's the highest!


```{r}
data_final %>%
  ggplot(aes(accommodates, price)) + geom_point() + labs(title = "Number of People a Listing Accomodates vs Price") + geom_smooth()
```

When plotting price vs accomodates, we see a weak positive correlation, until about the 12th person. The data points for 16+ person listings show a lower price, which may be indicative of large houses outside NYC (cheaper areas). Regardless, this relationship between accomodates and price indicates a nonlinear relationship, which justifies our choice of using GAM compared to other parametric methods.

```{r}
data_final%>%
  ggplot(aes(bathrooms, price)) + geom_point(stat = 'identity') + 
  theme_minimal() + 
  labs(title = "Bathrooms vs Price?",x = "Number of Bathrooms", y = "Price") + 
  geom_smooth(method = 'lm')


```
Maybe geom_smooth() is a little unecessary here, but it's intuitive that the relationship between the number of bathrooms and the price isn't necessarily linear - a 1 bathroom overlooking the Hudson river is going to be much more expensive than a 2 bathroom place in Queens or Brooklyn. While there's an arguement to be made that this could still be modelled by a polynomial function, but again, GAM places no assumptions on the shape of the data unlike parametric models.

#Model Making:

Based on the visualizations above, it's quite clear that there are nonlinear trends that exist in our data. As this is a regression task as well, we decided to go with GAM, as it's flexible, and makes no assumptions about the relationships in our data. 

```{r, warning=FALSE, cache=TRUE}
set.seed(253)
    # Run the GAM
    gam_model <- train(
      price ~.,
      data = data_final,
      method = "gamLoess",
      tuneGrid = data.frame(span = seq(0.1, 1, length = 10), degree = 1),
      trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
      metric = "MAE",
      na.action = na.omit
    )
    
  
```

    ```{r}
    # Plot CV MAE under different spans
    
    plot(gam_model)
    plot(gam_model, ylim=c(0,100))
    
    # ID best span parameter
    gam_model$bestTune
    
    # Calculate the CV MAE of the best model
    gam_model$results %>% 
      filter(span == gam_model$bestTune$span)
    gam_model$resample %>% 
      summarize(mean(MAE))
    gam_model$resample %>%
      summarize(mean(Rsquared))
    ```
As we can see in this graph our CV MAE for our model drops significantly at 0.25 span, and it remains close to a similar MAE value after the span is 0.5 and onwards. But the variation of CV MAE accross the model is very high. And due to the variation in CV being so significant we decided to use "best" selection function to calculate our span value of 0.6. 


```{r}
set.seed(253)
    least_squares <- train(
      price ~.,
      data = data_final,
      method = "lm",
      trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
      metric = "MAE",
      na.action = na.omit
    )

    
mean(least_squares$resample$MAE)
least_squares$resample

mean(least_squares$resample$Rsquared)


```

```{r}
mean(gam_model$resample$Rsquared)
```


```{r}
    # Put the 11 plots in a 4x3 grid
    par(mfrow = c(4,3))
    
    # Make plots
    plot(gam_model$finalModel)
```


Using GAM, we can see a mix of relationships between price and variables. Variables like minimum_nights appear to have a linear (and thus, parametric) relationship with price, while number_of_reviews appears to have a non-parametric, or highly polynomial relationship with price, all else held constant. Clearly, based on least squares, most variables must have a parametric relationship, or else least squares would not have preformed so similarity to GAM. 








## Part 3: Summarize



\
\
\
\
\
\

**Well!**

Despite our intution that we should use GAM due to nonparametric relationships between Price and various variables in the dataset, the mean MAE of both GAM and least squares are very similar. Least squares has a slightly higher mean R^2 value, while GAM has a slightly lower mean MAE value than least squares. This indicates that GAM is definetly more flexible, and thus produces predictions with higher accuracy, but somehow fails to account for the same or more variation in price.

As a result, if we are looking to make predictions on price, we'd reccomend GAM. If we are looking to simply explain the variation within the data, we reccomend using least squares. However, if model interpretability and complexity are concerns, we'd reccomend simply going with the least squares model over the GAM. 

```{r}
rbind(mean(gam_model$resample$Rsquared),mean(least_squares$resample$Rsquared)) 
```

```{r}
rbind(mean(gam_model$resample$MAE),mean(least_squares$resample$MAE)) 
```

**probably delete this***
As mentioned before, we had strong intuition that the relationship between price and some of the variables was non-parametric. Looking at host_response_rate2 or property_type, for example, we can see the weird kinks in the shape of the model that indicate our decision to go with a nonparametric model was a correct one. However, the similarity of the least squares and GAM model indicates to us that all the variables have a parametric relationship with price, when all other variables are held constant.  



**but not this**
*What did we try?*
We first ran KNNImpute on all the variables that had missing NAs, but quickly realized this was not a good plan of action, given taking the mean of certain variables did not make sense. E.g, taking the mean for a categorical encoded variable makes no sense. Therefore to fill the NA's we put the most frequent value for the categorical variables, and for the quantitative variables in our data set we chose the ones that had fewer NA's. We then ran GAM on the variables we selected using LASSO, but playing devil's advocate, we decided to try least squares to see what we would get. Interestingly, the least squares model preformed much better than we expected, as shown above. 



## Part 4: Contributions

Josh: Visualizations and GAM
KY: LASSO
Niketh: Summary
Fed: Helped clean up data and fix NA's

